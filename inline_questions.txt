一、knn

Q1.Notice the structured patterns in the distance matrix, where some rows or columns are visibly brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)
- What in the data is the cause behind the distinctly bright rows?
- What causes the columns?

A：
- 可能是这张图片和训练集里面所有的图片都相差太大，或者根本不属于训练集中的这些类别
- 列表示的是训练集中的某一张图片与测试集中的所有图片的距离， 白色的列表示训练集中的这张图片和测试集中的都相差很大


Q2.We can also use other distance metrics such as L1 distance.
For pixel values $p_{ij}^{(k)}$ at location $(i,j)$ of some image $I_k$, 

the mean $\mu$ across all pixels over all images is $$\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}$$
And the pixel-wise mean $\mu_{ij}$ across all images is 
$$\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.$$
The general standard deviation $\sigma$ and pixel-wise standard deviation $\sigma_{ij}$ is defined similarly.

Which of the following preprocessing steps will not change the performance of a Nearest Neighbor classifier that uses L1 distance? Select all that apply.
1. Subtracting the mean $\mu$ ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu$.)
2. Subtracting the per pixel mean $\mu_{ij}$  ($\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}$.)
3. Subtracting the mean $\mu$ and dividing by the standard deviation $\sigma$.
4. Subtracting the pixel-wise mean $\mu_{ij}$ and dividing by the pixel-wise standard deviation $\sigma_{ij}$.
5. Rotating the coordinate axes of the data.

A:
1:每次计算的时候将每一个像素减去同一个平均值，L1距离不会发生任何变化，所以不会影响。 
2:每次计算的时候将每一个像素减去不同图片像素的平均值，L1距离会发生变化。 
3:和1的原理一样，减去同一个平均值后，再处以同一个标准差，不影响L1的值。 
4:和2的原理一样，L1距离发生变化。 
5:当坐标轴发生旋转时，L1距离会变化，只有L2距离不会发生变化，


Q3:
Which of the following statements about $k$-Nearest Neighbor ($k$-NN) are true in a classification setting, and for all $k$? Select all that apply.
1. The decision boundary of the k-NN classifier is linear.
2. The training error of a 1-NN will always be lower than or equal to that of 5-NN.
3. The test error of a 1-NN will always be lower than that of a 5-NN.
4. The time needed to classify a test example with the k-NN classifier grows with the size of the training set.
5. None of the above.

A：
1. 不一定
2. 不一定
3. 不一定
4. 正确


二、svm
Q1:It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? How would change the margin affect of the frequency of this happening? *Hint: the SVM loss function is not strictly speaking differentiable*
A:
数值解是用前后2个很小的尺度进行计算，但是Syi=Sj+１的时候，Loss不可导


Q2：Describe what your visualized SVM weights look like, and offer a brief explanation for why they look the way they do.
A：
可视化的结果类似是每个不同类的模板，由于这些模板是从训练集中提取出来的，所以受到训练集的影响。


三、softmax
Q1:Why do we expect our loss to be close to -log(0.1)? Explain briefly.**
A:
首先W因为非常小，所以正则化的影响可以忽略不计； 总共有十个类别，在训练集比较均匀的状态下，exp(yi)/sum(exp(j))应该接近于0.1

Q2:Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.
A:正确
如果新加入的测试图片分类正确，则SVM地loss函数计算结果一定为0；
但是对与softmax而言，不论分类是否正确，loss总会是存在的，即使loss趋近于0，损失值也变化了。


四、two-layer-net
Q1:We've only asked you to implement ReLU, but there are a number of different activation functions that one could use in neural networks, each with its pros and cons. In particular, an issue commonly seen with activation functions is getting zero (or close to zero) gradient flow during backpropagation. Which of the following activation functions have this problem? If you consider these functions in the one dimensional case, what types of input would lead to this behaviour?
1. Sigmoid
2. ReLU
3. Leaky ReLU

A:
sigmoid会出现正饱和和负饱和，当输入值很大（正的）或者很小（负的）时，导数趋于一致；
ReLU在负半轴也会出现这种问题。

Q2：Now that you have trained a Neural Network classifier, you may find that your testing accuracy is much lower than the training accuracy. In what ways can we decrease this gap? Select all that apply.
1. Train on a larger dataset.
2. Add more hidden units.
3. Increase the regularization strength.
4. None of the above.

A：1和3。 增加隐藏节点可能会导致过拟合问题。 


五、features
Q1：Describe the misclassification results that you see. Do they make sense?

A：
有部分被错误分类的图片还是挺像的，比如car被分到了truck中，但是大部分被分错类的图片与原油分类相差很大，这可能是由于提取的特征还不够很好地区分这些类别。

